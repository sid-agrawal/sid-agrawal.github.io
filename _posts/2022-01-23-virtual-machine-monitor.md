# Virtual Machine Core Idea

While trying to find tutorials on how to implmenet a basic hypervisor, most of the articles I found online were about how to use Intelâ€™s VT-X extensions - called `hardware extensions for virtualization` to implment a Virtual machine [1]. However, the idea of virtual machine is so much older than hardware extensions meant to make virtualization easier. So, I wanted to get to the crux of what is it that we are virtualizing and how. Then later, see how hardware extensions for virtualization make it either easier or speed things up. 

I realized that the answer lies in how host OS deals with faults inside the process. If the faults are handled a certain way, the process is running a normal application. If the fault is handled in a slightly more complicated way - more on that below - it is a virtual machine. 

# Running an application

When I am running a normal application in the process - and not a VM - the OS needs to handle faults generated by the process, and it has to forward the appropriate interrupts to the process.

```c
switch exception_type:
		  case syscall:
					do the right thing
					syscall_ret // This will start the user process at the next instruction.
		  case page_fault:
					map the page 
					restart from the faulting instruction
			default:
					kill the process
```

## Run a VM with the same arch and no hardware extensions.

In this scenario, the process is running a virtual machine. So, when the process executes a 

privileged instruction the OS - we will say host OS going forward - will have handle that gracefully.

But!! Depening on whether the virtual machine is running the guestOS or a user-process at a given instant the response from the OS will have to be different. Thatâ€™s why the host OS needs to maintain additional state about the CPU running the VM. This is denoted by the dictionary `cpu_state` below.

Further more, there are certain action that the  processor does automatically, which have be mimicked by the host OS. For instance, when a user process issues a system call. The process automatically switches the address-space and sets the kernel bit in the CPU. Similarly, when the kernel returns from the system call, the hardware automaticaly swtiches back to the userâ€™s address space and unsets the kernel bit in the the CPU. 

Systems calls and page faults are exceptions generated inside the VM going outward. But we also need to consider how interrupts generated outside the VM will be guided to the guestOS or the process inside the VM. When an OS boots up - or some early on - it sets up the interrupt vector table, which dictates where is the handlers for each type of interrup. When the guest OS is setting up the interrupt vector table, the host OS will intercept these requests and instead maintain a shadown IVT. Then when an interrupt needs to be sent to the VM, the host OS can change the IP of the guest OS to that address.

```c
// For each CPU, we need to maintain some state 
dict cpu_state_type {
   kernel_mode: true,
	 cr3: bit-value,
	[...]  // other privileaged state
}

switch fault_type:
		  case syscall:
					if vCPU_in_kernel_mode() {
						kill it // This shouldn't have happened, when the CPU is in kernel mode.
					} else {
							set_vCPU_kernel_mode()
							restart app from kernel's syscall handler
					}
		  case syscall_ret:
					if vCPU_in_kernel_mode() {
						unset_vCPU_kernel_mode()
						restart app from some where else?? // need to find out the name of the reg where the return addrs is stored.
					} else {
							kill it // this shouldn't have happened.
					}
			case update_cr3:
					if vCPU_in_kernel_mode() {
						update_cr3
            restart app from next instruction
					} else {
							kill it // this shouldn't have happened.		
					}
 		case page_fault:
					if vCPU_in_kernel_mode() {
						handle fault
            restart VM from same instruction
					} else {
							set kernel mode bit 
							restart VM with IP set to guestOS's page fault handler
					}

			default:
					kill the process
```

# Run a VM on the same arch and with hardware extensions.

In Intel with VT-X , there root and non-root mode. The HV is meant to run in the root mode and the guest OS in the non-root mode. In the non root mode, most the most part the guest OS is able to proceed as it was running on bare-metal. Access to privelaged state like CR3 register are not trapped to the HV. 

This would mean that when the guest OS is executing, very few things will cause the hypervisor to get involved.

```c
dict cpu_state_type1{
   kernel_mode: true,
	 cr3: bit-value,
	[...]  // other priv state
}

switch fault_type:
		  case syscall:
					if vCPU_in_kernel_mode() {
						kill it // This shouldn't have happened.
					} else {
							set_vCPU_kernel_mode()
							restart app from kernel's syscall handler
					}
		  case syscall_ret:
					if vCPU_in_kernel_mode() {
						kill it // This shouldn't have happened.
					} else {
							unset_vCPU_kernel_mode()
							restart app from some where else??
					}
			case update_cr3:
					if vCPU_in_kernel_mode() {
						update_ce3
					} else {
							restart app from same place
					}

 		case page_fault:
					if vCPU_in_kernel_mode() {
						handle fault
            restart VM from same instruction
					} else {
							set kernel mode bit 
							restart VM with IP set to guestOS's page fault handler
					}
			default:
					kill the process
```

<aside>
ðŸ’¡ What about interrupts going into the VM.

</aside>

- Just set the correct interrup bit in the vCPU struct. Which the guest OS would read? naah, that sounds wrong.
- When an interrupt is generated by the HW the VMM will used the IP of a previously entered entry to the Interrupt Vector Table and jump the OS there.

vCPU-Type2: Different arch: every instruction will trap to kernel.

[1] [https://nixhacker.com/developing-hypervisior-from-scratch-part-1/](https://nixhacker.com/developing-hypervisior-from-scratch-part-1/)